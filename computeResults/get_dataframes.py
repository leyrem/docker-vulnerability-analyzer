import pandas as pd
import json
import glob
import os
from concurrent.futures import ProcessPoolExecutor
from utils import process_file, parse_unique_cves_field
import numpy as np

# Paths
DIR_OFFICIAL = '/root/docker-vulnerability-analyzer/out/analysis/official/'
DIR_VERIFIED = '/root/docker-vulnerability-analyzer/out/analysis/verified/'
DIR_SPONSORED = '/root/docker-vulnerability-analyzer/out/analysis/sponsored/'

num_images_analysed = 0

# Load data
df_official = pd.read_csv('/root/thesis_crawler/imagesOfficial.csv')
df_verified = pd.read_csv('/root/thesis_crawler/imagesVerified.csv')
df_sponsored = pd.read_csv('/root/thesis_crawler/imagesSponsored.csv')

SELECTED_NAME = "Official Images" # Oneof Official Images, Verified Images, Sponsored Images or All Images

directory = ''
if SELECTED_NAME == "Official Images":
    directory = DIR_OFFICIAL
elif SELECTED_NAME == "Verified Images":
    directory = DIR_VERIFIED
elif SELECTED_NAME == "Sponsored Images":
    directory = DIR_SPONSORED

df_cves_detected_all_scanners = pd.DataFrame()
df_unique_cves = pd.DataFrame()
num_cves_list = []

iii = 1
# Load dataframes
if directory == '': # Case ALL IMAGES
    list_dirs = [DIR_OFFICIAL, DIR_VERIFIED, DIR_SPONSORED]
    num_images_analysed = 0
    for d in list_dirs:
        pattern = os.path.join(d, '*')
        file_list = [filename for filename in glob.glob(pattern) if os.path.isfile(filename)]
    
        with ProcessPoolExecutor() as executor:
            results = list(executor.map(process_file, file_list))
            
        # Combine results
        for df1, df2, num in results:
            num_cves_list.append(num)
            df_cves_detected_all_scanners = pd.concat([df_cves_detected_all_scanners, df1], ignore_index=True)
            df_unique_cves = pd.concat([df_unique_cves, df2], ignore_index=True)
            num_images_analysed += 1
            print(f"Loading data {iii}")
            iii += 1

else:    
    pattern = os.path.join(directory, '*')
    num_images_analysed = 0
    file_list = [filename for filename in glob.glob(pattern) if os.path.isfile(filename)]
    
    with ProcessPoolExecutor() as executor:
        results = list(executor.map(process_file, file_list))
        
    # Combine results
    for df1, df2, num in results:
        num_cves_list.append(num)
        df_cves_detected_all_scanners = pd.concat([df_cves_detected_all_scanners, df1], ignore_index=True)
        df_unique_cves = pd.concat([df_unique_cves, df2], ignore_index=True)
        num_images_analysed += 1
        print(f"Loading data {iii}")
        iii += 1
        
# Save the DataFrame to a CSV file
name1 = "df_cves_detected_all_scanners_" + SELECTED_NAME + ".csv"
name2 = "df_unique_cves_" + SELECTED_NAME + ".csv"
#df_cves_detected_all_scanners.to_csv(name1, index=False)
#df_unique_cves.to_csv(name2, index=False)

# Convert to numpy array
data_array = np.array(num_cves_list)

# Compute statistics
average_cves_per_image = np.mean(data_array).item()
median_cves_per_image = np.median(data_array).item()
variance_cves_per_image = np.var(data_array, ddof=1).item()  # ddof=1 for sample variance
std_dev_cves_per_image = np.std(data_array, ddof=1).item()  # ddof=1 for sample standard deviation
min_value_cves_per_images = np.min(data_array).item()
max_value_cves_per_image = np.max(data_array).item()
range_value_cves_per_image = max_value_cves_per_image - min_value_cves_per_images

data_output = {
    "executing_script" : "get_dataframes.py",
    "images_analysed_type": SELECTED_NAME,
    "number_images_analysed": num_images_analysed,
    "len_num_cves_list": len(num_cves_list),
    "average_cves_per_image": average_cves_per_image,
    "median_cves_per_image": median_cves_per_image,
    "variance_cves_per_image": variance_cves_per_image,
    "std_dev_cves_per_image": std_dev_cves_per_image,
    "min_value_cves_per_images": min_value_cves_per_images,
    "max_value_cves_per_image": max_value_cves_per_image,
    "range_value_cves_per_image": range_value_cves_per_image,
}

json_file_name = 'out/general_stats_' + SELECTED_NAME + '.json'
with open(json_file_name, 'w') as json_file:
    json.dump(data_output, json_file, indent=4)