import streamlit as st
import pandas as pd
import altair as alt
import json
import glob
import os
from concurrent.futures import ProcessPoolExecutor, as_completed
import itertools

from utils import get_distribution_cves_per_scanner, parse_unique_cves_field, extract_severity, get_cve_details

DIR_OFFICIAL = '/root/docker-vulnerability-analyzer/out/analysis/official/'
DIR_VERIFIED = '/root/docker-vulnerability-analyzer/out/analysis/verified/'
DIR_SPONSORED = '/root/docker-vulnerability-analyzer/out/analysis/sponsored/'

# Page config
st.set_page_config(
    page_title="Docker Large Scale Image Vulnerability Analysis Dashboard",
    page_icon="🏂",
    layout="wide",
    initial_sidebar_state="expanded")

alt.themes.enable("dark")

# Load data
df_official = pd.read_csv('/root/thesis_crawler/imagesOfficial.csv')
df_verified = pd.read_csv('/root/thesis_crawler/imagesVerified.csv')
df_sponsored = pd.read_csv('/root/thesis_crawler/imagesSponsored.csv')


# Create dropdown widgets on a sidebar for users to select data
with st.sidebar:
    st.title('🏂 Docker Large Scale Image Vulnerability Analysis Dashboard')

    # Select a image repo name
    image_name_list = ["Official Images", "Verified Images", "Sponsored Images", "All Images"]
    selected_name = st.selectbox('Select a set of Docker images', image_name_list, index=len(image_name_list)-1)

    directory = ''
    if selected_name == "Official Images":
        directory = DIR_OFFICIAL
    elif selected_name == "Verified Images":
        directory = DIR_VERIFIED
    elif selected_name == "Sponsored Images":
        directory = DIR_SPONSORED

    df_cves_detected_all_scanners = pd.DataFrame()
    df_unique_cves = pd.DataFrame()

    if directory == '': # Case ALL IMAGES
        list_dirs = [DIR_OFFICIAL, DIR_VERIFIED, DIR_SPONSORED]

        for d in list_dirs:
            pattern = os.path.join(d, '*')
            for filename in glob.glob(pattern):
                if os.path.isfile(filename):
                    with open(filename) as file:
                        dataJSON = json.load(file)
                        
                    df1 = pd.json_normalize(dataJSON, 'cves_detected_all_scanners')
                    df2 = pd.json_normalize(dataJSON, 'unique_cves')
                    
                    df_cves_detected_all_scanners = pd.concat([df_cves_detected_all_scanners, df1], ignore_index=True)
                    df_unique_cves = pd.concat([df_unique_cves, df2], ignore_index=True)
    else:
        pattern = os.path.join(directory, '*')
        for filename in glob.glob(pattern):
            if os.path.isfile(filename):
                with open(filename) as file:
                    dataJSON = json.load(file)
                    
                df1 = pd.json_normalize(dataJSON, 'cves_detected_all_scanners')
                df2 = pd.json_normalize(dataJSON, 'unique_cves')
                df_cves_detected_all_scanners = pd.concat([df_cves_detected_all_scanners, df1], ignore_index=True)
                df_unique_cves = pd.concat([df_unique_cves, df2], ignore_index=True)
                
tab1, tab2 = st.tabs(["Image Information and Metadata", "Vulnerability Analysis"])

with tab1:
    st.header("Image Information and Metadata")
    
    # Display the filtered dataframe for debugging
    st.write("DataFrame CVES_detected_all_scanners:")
    st.dataframe(df_cves_detected_all_scanners)

    if len(df_unique_cves) > 0:
        st.write("DataFrame unique_cves:")
        st.write(df_unique_cves)
        
with tab2:
    st.header("Vulnerability Analysis")
    
    tab2_1, tab2_2, tab2_3 = st.tabs(["Overall Vulnerability Landscape", "Detailed Unique Vulnerabilities", "Scanner Performance Analysis"])
    
    with tab2_1:
        st.subheader("Overall Vulnerability Landscape", divider='rainbow')
        
        
        unique_CVEs_all = {}
        
        for index, row in df_unique_cves.iterrows():
            detection_rate = row['detection_rate']
            scanner_cve_inf = row['scanner_cve_info']
            scanner_cve_inf_df = parse_unique_cves_field(scanner_cve_inf)
            cve_id = scanner_cve_inf_df['cve_id'].iloc[0]
            
            if cve_id in unique_CVEs_all:
                unique_CVEs_all[cve_id].append(scanner_cve_inf_df)
            else:
                unique_CVEs_all[cve_id] = [scanner_cve_inf_df]
            
        st.metric(label="Total number of unique CVEs found", value=len(unique_CVEs_all))
        cves_freq_df = pd.DataFrame(columns=['CVE ID', 'Images Present Count'])
    
   
        num_low_vulns = 0
        num_medium_vulns = 0
        num_high_vulns = 0
        num_critical_vulns = 0
        num_unassigned_vulns = 0
        num_diff_vulns = 0
        num_reports_unmatch = 0

        i = 1
        for key, value in unique_CVEs_all.items():
            single_row = pd.DataFrame([{'CVE ID': key, 'Images Present Count': len(value)}])
            cves_freq_df = pd.concat([cves_freq_df, single_row], ignore_index=True)

            value_sev_set = set()
            
            print(f"Doing loop {i} out of: {len(unique_CVEs_all)}")

            i += 1
            
            # PARELLISE LOOP
            with ProcessPoolExecutor() as executor:
                #futures = [executor.submit(extract_severity, key, scanner_cve_info_obj_df, False) for scanner_cve_info_obj_df in value]
                futures = list(executor.map(extract_severity, itertools.repeat(key), value, itertools.repeat(False)))

                for future in as_completed(futures):
                    value_sev_set.add(future)
        
            #for scanner_cve_info_obj_df in value:
                #value_sev = extract_severity(key, scanner_cve_info_obj_df, False)
                #value_sev_set.add(value_sev)
                
            if len(value_sev_set) == 0:
                st.write("ERROR OCCURRED WHILE ASSESING SEVERITIES, NO VALUE ASSIGNED")
            elif len(value_sev_set) == 1:
                val = list(value_sev_set)[0]
                if val == "LOW":
                    num_low_vulns += 1
                elif val == "MEDIUM":
                    num_medium_vulns += 1
                elif val == "HIGH":
                    num_high_vulns += 1
                elif val == "CRITICAL":
                    num_critical_vulns += 1
                elif val == "UNASSIGNED":
                    num_unassigned_vulns += 1
                elif val == "DIFFERING":
                    num_diff_vulns += 1
            else:
                num_reports_unmatch += 1
                #st.write("DIFFERENT SEVERITIES ASSIGNED FOR THE SAME CVE IN DIFFERENT IMAGE RESULTS")
                
            
        col1, col2, col3 = st.columns(3)
                 
        col1.metric(label ="Number of CRITICAL vulnerabilities", value=num_critical_vulns)
        col2.metric(label ="Number of HIGH vulnerabilities", value=num_high_vulns)
        col3.metric(label ="Number of MEDIUM vulnerabilities", value=num_medium_vulns)
        
        col4, col5, col6 = st.columns(3)
        col4.metric(label ="Number of LOW vulnerabilities", value=num_low_vulns)
        col5.metric(label ="Number of UNASSIGNED vulnerabilities", value=num_unassigned_vulns)
        col6.metric(label ="Number of vulnerabilities with different severity levels reported", value=num_diff_vulns)
        
        
        st.metric(label = "Number of vulns with reports unmatching", value=num_reports_unmatch)
        
        
        ## REPEAT RECOM OF SEVERITIES
        
        st.subheader("AFTER RECOM")
        
        num_low_vulns = 0
        num_medium_vulns = 0
        num_high_vulns = 0
        num_critical_vulns = 0
        num_unassigned_vulns = 0
        num_diff_vulns = 0
        num_reports_unmatch = 0

        for key, value in unique_CVEs_all.items():
            value_sev_set = set()
            
            # PARELLISE LOOP
            with ProcessPoolExecutor() as executor:
                futures = [executor.submit(extract_severity, key, scanner_cve_info_obj_df, True) for scanner_cve_info_obj_df in value]

                for future in as_completed(futures):
                    value_sev_set.add(future)
        
            #for scanner_cve_info_obj_df in value:
                #value_sev = extract_severity(key, scanner_cve_info_obj_df, True)
                #value_sev_set.add(value_sev)
                
            if len(value_sev_set) == 0:
                st.write("ERROR OCCURRED WHILE ASSESING SEVERITIES, NO VALUE ASSIGNED")
            elif len(value_sev_set) == 1:
                val = list(value_sev_set)[0]
                if val == "LOW":
                    num_low_vulns += 1
                elif val == "MEDIUM":
                    num_medium_vulns += 1
                elif val == "HIGH":
                    num_high_vulns += 1
                elif val == "CRITICAL":
                    num_critical_vulns += 1
                elif val == "UNASSIGNED":
                    num_unassigned_vulns += 1
                elif val == "DIFFERING":
                    num_diff_vulns += 1
            else:
                num_reports_unmatch += 1
                #st.write("DIFFERENT SEVERITIES ASSIGNED FOR THE SAME CVE IN DIFFERENT IMAGE RESULTS")

        col1, col2, col3 = st.columns(3)
                 
        col1.metric(label ="Number of CRITICAL vulnerabilities", value=num_critical_vulns)
        col2.metric(label ="Number of HIGH vulnerabilities", value=num_high_vulns)
        col3.metric(label ="Number of MEDIUM vulnerabilities", value=num_medium_vulns)
        
        col4, col5, col6 = st.columns(3)
        col4.metric(label ="Number of LOW vulnerabilities", value=num_low_vulns)
        col5.metric(label ="Number of UNASSIGNED vulnerabilities", value=num_unassigned_vulns)
        col6.metric(label ="Number of vulnerabilities with different severity levels reported", value=num_diff_vulns)
        
        
        st.metric(label = "Number of vulns with reports unmatching", value=num_reports_unmatch)
        
        ## done-------------------------------------
        
        cves_freq_df_sorted = cves_freq_df.sort_values(by='Images Present Count', ascending=False)
        st.dataframe(cves_freq_df_sorted)
        
        